\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[UKenglish]{isodate}
\origdate
\cleanlookdateon
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{tikz-cd}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{tikz, venndiagram}
\usepackage{tkz-euclide}
\usepackage{tabu}
\renewcommand{\baselinestretch}{1.5} 

%%% Better lambda 
\usepackage{pifont}
\makeatletter
\newcommand\Pimathsymbol[3][\mathord]{%
  #1{\@Pimathsymbol{#2}{#3}}}
\def\@Pimathsymbol#1#2{\mathchoice
  {\@Pim@thsymbol{#1}{#2}\tf@size}
  {\@Pim@thsymbol{#1}{#2}\tf@size}
  {\@Pim@thsymbol{#1}{#2}\sf@size}
  {\@Pim@thsymbol{#1}{#2}\ssf@size}}
\def\@Pim@thsymbol#1#2#3{%
  \mbox{\fontsize{#3}{#3}\Pisymbol{#1}{#2}}}
\makeatother
% the next two lines are needed to avoid LaTeX substituting upright from another font
\input{utxmia.fd}
\DeclareFontShape{U}{txmia}{m}{n}{<->ssub * txmia/m/it}{}
% you may also want
\DeclareFontShape{U}{txmia}{bx}{n}{<->ssub * txmia/bx/it}{}
% just in case
%\DeclareFontShape{U}{txmia}{l}{n}{<->ssub * txmia/l/it}{}
%\DeclareFontShape{U}{txmia}{b}{n}{<->ssub * txmia/b/it}{}
% plus info from Alan Munn at https://tex.stackexchange.com/questions/290165/how-do-i-get-a-nicer-lambda?noredirect=1#comment702120_290165
\newcommand{\pilambdaup}{\Pimathsymbol[\mathord]{txmia}{21}}
\pgfplotsset{compat=1.16}
\begin{document}
\title{Finals Revision Guide\\[0.1cm]
    \large 30.003 Probability and Statistics, Term 4 2019}
\author{Wei Min Cher}
\date{05 Jan 2020}

\maketitle

\tableofcontents

\newpage
\section{W8: Statistics and Their Distributions}
\subsection{Definitions}
\begin{itemize}
    \item Population: all observations
    \item Sample: subset of population
    \item Random sample: made up of random variables that are independently and identically distributed
    \item Statistic: quantity whose value can be calculated from sample data
    \begin{itemize}[label=$\circ$]
        \item A random variable
    \end{itemize}
\end{itemize}
\subsection{Order statistic}
For iid RVs $X_{1}, X_{2}, \ldots , X_{n}$ of unknown distribution, they can be rearranged in an increasing order:
$$X_{(1)}\leq X_{(2)}\leq \ldots X_{(k)} \ldots \leq X_{(n)}$$
where
\begin{itemize}
    \item $X_{(1)} = \min\{X_{1}, \ldots , X_{n}\}$ is the smallest order statistic;
    \item $X_{(k)}$ is the $k$-th order statistic; and
    \item $X_{(n)} = \max\{X_{1}, \ldots , X_{n}\}$ is the largest order statistic.
\end{itemize}
\subsection{Sample range}
The sample range $R$ is the distance between the largest and smallest order statistic.\\
It is also a random variable, and can be calculated by:
$$R = X_{(n)} - X_{(1)}$$
\subsection{Distribution of a statistic}
The distribution of a statistic can be obtained by either 1 of the 2 methods:
\begin{enumerate}
    \item Derive the probability distribution analytically via order statistics
    \item Simulate the probability distribution using Monte Carlo simulation
\end{enumerate}
\subsection{Distribution of \texorpdfstring{$\overline{X}$}{Xbar}}
For a sufficiently large $n$, i.e. $\mathbf{n \leq 30}$, $\overline{X}$ has approximately a normal distribution with mean $E(\overline{X})$ and variance $V(\overline{X})$ as follows:
\begin{itemize}
    \item Mean, $E(\overline{X}) = \mu$
    \item Variance, $\displaystyle V(\overline{X}) = \frac{\sigma^2}{n}$
\end{itemize}
\subsection{Distribution of smallest order statistic \texorpdfstring{$X_{(1)}$}{X(1)}}
\begin{itemize}
    \item pdf:
    $$f_{(1)}(x) = n\left[1-F_{X}(x)\right]^{n-1}f_{X}(x)$$
    \item cdf: \begin{align*}
    F_{(1)}(x) &= P(X_{(1)}\leq x)\\
    &= 1 - P(X_{i}> x, \forall i)\\
    &= 1 - \prod_{i=1}^{n}P(X_{i}> x) \quad \text{(independent)}\\
    &= 1 - \left[P(X_{i}> x)\right]^{n} \quad \text{(identically distributed)}\\
    &= 1-\left[1-P(X_{i}\leq x)\right]^{n}\\
    &= 1-\left[1-F_{X}(x)\right]^{n}
\end{align*}
\end{itemize}
\subsection{Distribution of largest order statistic \texorpdfstring{$X_{(n)}$}{X(n)}}
\begin{itemize}
    \item pdf:
    $$f_{(n)}(x) = n\left[F_{X}(x)\right]^{n-1}f_{X}(x)$$
    \item cdf: \begin{align*}
    F_{(n)}(x) &= P(X_{(n)}\leq x)\\
    &= P(X_{i}\leq x, \forall i)\\
    &= \prod_{i=1}^{n}P(X_{i}\leq x) \quad \text{(independent)}\\
    &= \left[P(X_{i}\leq x)\right]^{n} \quad \text{(identically distributed)}\\
    &= \left[F_{X}(x)\right]^{n}
\end{align*}
\end{itemize}
\subsection{Distribution of \texorpdfstring{$k$-th}{k-th} order statistic \texorpdfstring{$X_{(k)}$}{X(k)}}
\begin{itemize}
    \item pdf:
    $$f_{(k)}(x) = \frac{n!\left[F_{X}(x)\right]^{k-1}\left[1-F_{X}(x)\right]^{n-k}f_{X}(x)}{(k-1)!(n-k)!}$$
\end{itemize}
\newpage
\section{W9: Point Estimation}
\subsection{Point estimate}
\begin{itemize}
    \item Statistic, function of data to infer value of unknown parameter
    \item A random variable
    \begin{itemize}[label=$\circ$]
        \item e.g. point estimate of $\theta$ is $\hat{\theta}$
    \end{itemize}
\end{itemize}
\subsection{Principle of Unbiased Estimator}
\begin{itemize}
    \item Choose an unbiased estimator among several candidates
    \item Point estimate $\hat{\theta}$ is an unbiased estimator if $E(\hat{\theta}) = \theta$ for every possible value of $\theta$
    \item Can be obtained from biased estimator by using making $E(\hat{\theta}) = \theta$
\end{itemize}
\subsection{Principle of Minimum Variance Unbiased Estimation}
\begin{itemize}
    \item Among all the unbiased estimators of $\theta$, choose the estimator with the minimum variance.
    \item Estimator with the minimum variance is the minimum variance unbiased estimator (MVUE) of $\theta$.
\end{itemize}
\newpage
\section{W9: Method of Moments Estimator (MME)}
\subsection{Moments}
\begin{itemize}
    \item $k$-th population moment, $\mu_{k} = E(X^k)$
    \begin{itemize}[label=$\circ$]
        \item Depends on unknown parameters
    \end{itemize}
    \item $k$-th sample moment, $\displaystyle M_{k} = \frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}$
    \begin{itemize}[label=$\circ$]
        \item Function of random sample
    \end{itemize}
\end{itemize}
\subsection{Method of Moments}
\begin{itemize}
    \item Assumes that sample moments provide good estimates of the corresponding population moments
\end{itemize}
\subsection{Method of Moments Estimator (MME)}
To calculate the MME(s) of $\theta$:
\begin{enumerate}
    \item Find $m$ population moments, where $m$ is the number of unknown parameters.
    \item Find $m$ sample moments.
    \item Equate each population moment to its corresponding sample moments
    \item Solve for $\theta = (\theta_{1}, \ldots, \theta_{m})$ to obtain the MMEs for $\theta$.
\end{enumerate}
\newpage
\section{W10: Maximum Likelihood Estimator (MLE)}
\subsection{Likelihood function}
Let $X_{1}, X_{2}, \ldots , X_{n}$ have a joint pdf or pmf:
$$L(\theta_{1}, \ldots, \theta_{m}) = f(x_{1}, \ldots, x_{n}; \theta_{1}, \ldots, \theta_{m})$$
The likelihood function is given by
\begin{align*}
    L(\theta) = P(X_{1}=x, \ldots, X_{n}=x_{n}) = \begin{dcases}
    \quad \prod_{i=1}^{n} p(x_{i}, \theta) &\mbox{for discrete RVs}\\
    \quad \prod_{i=1}^{n} f(x_{i}, \theta) &\mbox{for continuous RVs}
    \end{dcases}
\end{align*}
\subsection{Maximizing the likelihood}
\begin{itemize}
    \item The maximum likelihood estimator (MLE) $\hat{\theta}_{1}, \ldots, \hat{\theta}_{m}$ are values that maximize the likelihood function such that
    $$L(\hat{\theta}_{1}, \ldots, \hat{\theta}_{m})\leq L(\theta_{1}, \ldots, \theta_{m})
    $$
\end{itemize}
\subsection{Maximum Likelihood Estimator (MLE)}
To calculate the MLE of $\theta$:
\begin{enumerate}
    \item Find the likelihood function $L(\theta)$ based on the distribution.
    \item Differentiate $L(\theta)$ with respect to $\theta$, and equate the derivative to 0.
    \begin{itemize}[label=$\circ$]
        \item The natural logarithm of $L(\theta)$ could simplify calculations.
    \end{itemize}
    \item Solve for the MLE of $\theta$.
    \item Check if the value is maximum by taking the second derivative of $L(\theta)$.
\end{enumerate}
\noindent\textbf{Notes:}
\begin{itemize}
    \item In some cases, calculus-based techniques are not applicable to maximize likelihood function.
    \item MLE does not guarantee to produce an unbiased estimator.
\end{itemize}
\newpage
\section{W10: Confidence Interval}
\begin{itemize}
    \item Quantifies the confidence interval of a point estimate $\hat{\theta}$
    $$l(X_{1}, \ldots, X_{n})< \hat{\theta}(X_{1}, \ldots, X_{n})<u(X_{1}, \ldots, X_{n})$$
    \begin{itemize}[label=$\circ$]
        \item where $l(\ldots)$ is the lower bound and $u(\ldots)$ is the upper bound respectively. 
    \end{itemize}
    \item The interval contains $\theta$ with a confidence interval $p$:
    $$P\{\theta\in \left[l(X_{1},\ldots, X_{n}), u(X_{1}, \ldots, X_{n})\right] \} = p
    $$
    \item The confidence interval $p$ is often set to a high value e.g. 0.95, 0.99 in practice
\end{itemize}
\subsection{Equivalent expressions for Confidence Interval}
The following expressions are equivalent in describing a 90\% confidence interval (CI) for $\mu$.
\begin{align*}
    P\left(|\overline{X}-\mu| < \frac{1.65\sigma}{\sqrt{n}}\right) &= 0.90\\
    P\left(\overline{X}-\frac{1.65\sigma}{\sqrt{n}}<\mu<\overline{X}+\frac{1.65\sigma}{\sqrt{n}}\right) &= 0.90\\
    P\left[\mu\in\left(\overline{X}-\frac{1.65\sigma}{\sqrt{n}},\overline{X}+\frac{1.65\sigma}{\sqrt{n}}\right)\right] &= 0.90
\end{align*}
\begin{itemize}
    \item Replace 1.65 with:
    \begin{itemize}[label=$\circ$]
        \item 1.96 if CI is 95\%
        \begin{itemize}[label=\tiny$\blacksquare$]
            \item Closest Z-score of area 0.97500 in standard normal table
        \end{itemize}
        \item 2.58 if CI is 99\%
        \begin{itemize}[label=\tiny$\blacksquare$]
            \item Closest Z-score of area 0.99500 in standard normal table
        \end{itemize}
        \item \textbf{Rule of thumb:} 
        \begin{itemize}[label=\tiny$\blacksquare$]
            \item Search for Z score of area $p+\frac{1-p}{2}$ in the standard normal table, where $p$ is the CI.
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{Interpretation of Confidence Interval}
\begin{itemize}
    \item e.g. 95\% CI for $\mu$
    \begin{itemize}[label=$\circ$]
        \item As the number of samples collected tend to infinity, 95\% of the samples will contain $\mu$.
    \end{itemize}
\end{itemize}
\newpage
\subsection{Properties of Confidence Interval}
\begin{itemize}
    \item As population variance $\sigma$ increases, the width of CI increases.
    \item As sample size $n$ increases, the width of CI decreases.
    \item As the confidence interval $p$ increases, the width of CI increases.
    \item At a fixed confidence interval,
    \begin{itemize}[label=$\circ$]
        \item Large width of CI $\rightarrow{}$ low precision
        \item Small width of CI $\rightarrow{}$ high precision
    \end{itemize}
\end{itemize}
\newpage
\section{W11: Hypothesis Testing 1}
\subsection{Statistical hypothesis}
\begin{itemize}
    \item A claim about values of parameters/form of probability distribution
\end{itemize}
\subsection{Null and Alternative Hypotheses}
\begin{itemize}
    \item Null hypothesis, $H_{0}$
    \begin{itemize}[label=$\circ$]
        \item Claim that is initially assumed to be true
        \item $H_{0}$ is \textbf{always} $H_{0}:\theta = \theta_{0}$
    \end{itemize}
    \item Alternative hypothesis, $H_{a}$
    \begin{itemize}[label=$\circ$]
        \item Claim that contradicts the null hypothesis $H_{0}$
        \item $H_{a}$ has 3 forms with implicit hypothesis
        \begin{itemize}[label=\tiny$\blacksquare$]
            \item $H_{a}: \theta>\theta_{0}$ (implicit hypothesis: $\theta \leq \theta_{0}$)
            \item $H_{a}: \theta<\theta_{0}$ (implicit hypothesis: $\theta \leq \theta_{0}$)
            \item $H_{a}:\theta \neq \theta_{0}$ (implicit hypothesis: $\theta = \theta_{0}$)
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{Hypothesis Testing}
\begin{itemize}
    \item Method to decide whether to accept or reject the null hypothesis, $H_{0}$
    \item Comprises 2 components:
    \begin{enumerate}[label=$\circ$]
        \item Test statistic
        \begin{itemize}[label=\tiny$\blacksquare$]
            \item Function of sample data to make a decision
        \end{itemize}
        \item Rejection region
        \begin{itemize}[label=$\circ$]
            \item Set of values for which the null hypothesis $H_{0}$ will be rejected
            \item If test statistic falls in rejection region, $H_{0}$ will be rejected
        \end{itemize}
    \end{enumerate}
\end{itemize}
\subsection{Errors in Hypothesis Testing}
\begin{itemize}
    \item Type I error ($\alpha$): Rejecting the null hypothesis $H_{0}$ when $H_{0}$ is true
        $$\alpha = P(\text{reject }H_{0}\mid H_{0}\text{ is true})$$
    \item Type II error ($\beta$): Accepting the null hypothesis $H_{0}$ when $H_{a}$ is true
        $$\beta= P(\text{accept }H_{0}\mid H_{a}\text{ is true})$$
    \item Good rejection region yields small $\alpha$ and $\beta$
    \begin{itemize}[label=$\circ$]
        \item Typical approach: specify largest value of $\alpha$ that can be tolerated,\\then back-calculate for the rejection region
    \end{itemize}
\end{itemize}
\subsection{Hypothesis Testing using Rejection Region}
\begin{enumerate}
    \item Figure out appropriate $H_{0}$ and $H_{a}$.
    \item Figure out appropriate test statistic.
    $$\overline{X} = \frac{1}{n}\sum X_{i} \quad \Longrightarrow \quad Z =
    \begin{dcases}
    \quad \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ known}\\
    \quad \frac{\overline{X}-\mu}{\frac{s}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ unknown}
    \end{dcases}
    $$
    \item Calculate the rejection region based on type I error/significance level $\alpha$:
     $$\alpha = P(\text{reject }H_{0}\mid H_{0}\text{ is true})$$
     \item Calculate the normalized sample mean $z$ using sample mean $\overline{x}$.
     $$
     z =
    \begin{dcases}
    \quad \frac{\overline{x}-\mu}{\frac{\sigma}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ known}\\
    \quad \frac{\overline{x}-\mu}{\frac{s}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ unknown}
    \end{dcases}
    $$
    \item Compare the normalized sample mean $z$ with the rejection region.\\Reject $H_{0}$ if $z$ falls in the rejection region. 
    \begin{itemize}
        \item $H_{a}: \mu < \mu_{0}$ (lower-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Rejection region: $Z < -z_{\alpha}$
        \end{itemize}
        \item $H_{a}: \mu > \mu_{0}$ (upper-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Rejection region: $Z > -z_{\alpha}$
        \end{itemize}
        \item $H_{a}: \mu \neq \mu_{0}$ (two-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Rejection region: $Z < -z_{\alpha/2} \ \cup  \ Z > z_{\alpha/2}$
        \end{itemize}
    \end{itemize}
\end{enumerate}
\newpage
\section{W11: Hypothesis Testing 2}
\subsection{Hypothesis Testing of Difference between 2 Populations}
\begin{enumerate}
    \item Figure out appropriate $H_{0}$ and $H_{a}$.
    \begin{align*}
        H_{0}: \mu_{1}-\mu_{2} = 0\\
        H_{a}: \mu_{1}-\mu_{2} \neq 0
    \end{align*}
    \item Figure out appropriate test statistic.
    $$
    \begin{array}{cc}
    \displaystyle \overline{X_{1}}-\overline{X_{2}} = \frac{1}{n}\sum (X_{1i}-X_{2i})
    \mbox{}\\
    \\
    \Longrightarrow \quad Z =
    \begin{dcases}
    \quad \frac{\overline{X_{1}}-\overline{X_{2}}}{\frac{\sigma}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ known}\\
    \quad \frac{\overline{X_{1}}-\overline{X_{2}}}{\frac{s}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ unknown}
    \end{dcases}
    \end{array}
    $$
    \item Calculate the rejection region based on type I error/significance level $\alpha$:
     $$\alpha = P(\text{reject }H_{0}\mid H_{0}\text{ is true})$$
     \item Calculate the normalized sample mean $z$ using sample mean $\overline{x_{1}}-\overline{x_{2}}$.
     $$
     z =
    \begin{dcases}
    \quad \frac{\overline{x_{1}}-\overline{x_{2}}}{\frac{\sigma}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ known}\\
    \quad \frac{\overline{x_{1}}-\overline{x_{2}}}{\frac{s}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ unknown}
    \end{dcases}
    $$
    \item Compare the normalized sample mean $z$ with the rejection region.\\Reject $H_{0}$ if $z$ falls in the rejection region. 
    \begin{itemize}
        \item $H_{a}: \mu < \mu_{0}$ (lower-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Rejection region: $Z < -z_{\alpha}$
        \end{itemize}
        \item $H_{a}: \mu > \mu_{0}$ (upper-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Rejection region: $Z > -z_{\alpha}$
        \end{itemize}
        \item $H_{a}: \mu \neq \mu_{0}$ (two-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Rejection region: $Z < -z_{\alpha/2} \ \cup  \ Z > z_{\alpha/2}$
        \end{itemize}
    \end{itemize}
\end{enumerate}
\subsection{P-value}
\begin{itemize}
    \item A probability, calculated assuming that $H_{0}$ is true, of obtaining a value of the test statistic at least as contradictory to $H_{0}$ as the value calculated from the available sample.
\end{itemize}
\subsection{Hypothesis Testing using P-value}
\begin{enumerate}
    \item Figure out appropriate $H_{0}$ and $H_{a}$.
    \item Calculate the test statistic value of sample $z$.
    $$
     z =
    \begin{dcases}
    \quad \frac{\overline{x}-\mu}{\frac{\sigma}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ known}\\
    \quad \frac{\overline{x}-\mu}{\frac{s}{\sqrt{n}}} &\mbox{population standard deviation $\sigma$ unknown}
    \end{dcases}
    $$
    \item Determine range of test statistic values as contradictory to $H_{0}$ as the above value of $z$.
    \begin{itemize}
        \item $H_{a}: \mu < \mu_{0}$ (lower-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Range: $Z < z$
        \end{itemize}
        \item $H_{a}: \mu > \mu_{0}$ (upper-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Range: $Z > z$
        \end{itemize}
        \item $H_{a}: \mu \neq \mu_{0}$ (two-tailed test)
        \begin{itemize}[label=$\circ$]
            \item Range: $Z > z \ \cup  \ Z < -z$
        \end{itemize}
    \end{itemize}
    \item Calculate probability of getting that range, assuming $H_{0}$ is true:
    \begin{itemize}
        \item $H_{a}: \mu < \mu_{0}$ (lower-tailed test)
        \begin{itemize}[label=$\circ$]
            \item P-value $= P(Z < z\mid H_{0}$ is true)
        \end{itemize}
        \item $H_{a}: \mu > \mu_{0}$ (upper-tailed test)
        \begin{itemize}[label=$\circ$]
            \item P-value $= P(Z > z\mid H_{0}$ is true)
        \end{itemize}
        \item $H_{a}: \mu \neq \mu_{0}$ (two-tailed test)
        \begin{itemize}[label=$\circ$]
            \item $P$-value $= P(Z > z \ \cup  \ Z < -z\mid H_{0}$ is true)
        \end{itemize}
    \end{itemize}
    \item Compare the $P$-value against the significance level $\alpha$.
    \begin{itemize}
        \item Reject $H_{0}$: $P$-value $\leq \alpha$
        \item Accept $H_{0}$: $P$-value $> \alpha$
    \end{itemize}
\end{enumerate}
\subsection{Comparison between Hypothesis Testing Methods}
\begin{itemize}
    \item The two procedures -- the rejection region method and $P$-value method -- are equivalent.
    \begin{itemize}[label=$\circ$]
        \item The same conclusion will be reached via either of the two procedures.
    \end{itemize}
\end{itemize}
\newpage
\section{W12: Linear Regression}
\subsection{Least-squares method}
\begin{itemize}
    \item Estimates unknown parameters of a function based on known data
\end{itemize}
\subsection{Estimating \texorpdfstring{$\beta_{0}$}{beta0} and \texorpdfstring{$\beta_{1}$}{beta1}}
\begin{enumerate}
    \item Define an error function to minimize.
    $$f(\hat{\beta_{0}}, \hat{\beta_{1}}) = \sum_{i=1}^{n}(y_{i}-\hat{\beta_{1}}x_{i}-\hat{\beta_{0}})^{2}
    $$
    \item Take the partial derivative of the error function with respect to $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ and solve for the unknowns.
    \begin{align*}
    \frac{\partial f}{\partial \hat{\beta_{1}}} = 0: &-2\sum (y_{i}-\hat{\beta_{1}}x_{i}-\hat{\beta_{0}})(-x_{i}) = 0\\
    & \sum x_{i}(y_{i}-\hat{\beta_{1}}x_{i}-\hat{\beta_{0}}) = 0\\
    \xRightarrow{}&\sum(\hat{\beta_{1}}x_{i}^{2} +\hat{\beta_{0}}x_{i}) = \sum(x_{i}y_{i})\\
    \mbox{}\\
    \frac{\partial f}{\partial \hat{\beta_{0}}} = 0: &-2\sum (y_{i}-\hat{\beta_{1}}x_{i}-\hat{\beta_{0}})(-1) = 0\\
    &\sum(y_{i}-\hat{\beta_{1}}x_{i}-\hat{\beta_{0}}) = 0\\
    \xRightarrow{} &\sum(\hat{\beta_{1}}x_{i}+\hat{\beta_{0}}) = \sum y_{i}\\
    \end{align*}
    $$
    \text{Design matrix of error function: }\sum_{i=1}^{n}
    \begin{bmatrix}
        x_{i}^{2} & x_{i}\\
        x_{i} & 1
    \end{bmatrix}
    \begin{bmatrix}
        \hat{\beta_{1}}\\
        \hat{\beta_{0}}
    \end{bmatrix}
    =
    \sum_{i=1}^{n}
    \begin{bmatrix}
        x_{i}y_{i}\\
        y_{i}
    \end{bmatrix}
    $$
    \item Examine the Hessian matrix to determine if the solutions are at a minimum, i.e.
    $$
    \begin{bmatrix}
        \displaystyle \frac{\partial f}{\partial \hat{\beta_{0}^{2}}} \ & \displaystyle \frac{\partial^{2} f}{\partial \hat{\beta_{0}}\hat{\beta_{1}}} \\[12pt]
        \displaystyle \frac{\partial^{2} f}{\partial \hat{\beta_{0}}\hat{\beta_{1}}} & \displaystyle \frac{\partial f}{\partial \hat{\beta_{1}^{2}}}
    \end{bmatrix}\text{ is positive definite.}
    $$
\end{enumerate}
\subsection{Least-squares estimates for \texorpdfstring{$\beta_{0}$}{beta0} and \texorpdfstring{$\beta_{1}$}{beta1}}
\begin{align*}
    \hat{\beta_{0}} &= \overline{y}-\hat{\beta_{1}}\overline{x}\\
    \hat{\beta_{1}} &= \frac{S_{xy}}{S_{xx}} = \frac{\sum x_{i}y_{i}-n\bar{x}\bar{y}}{\sum x_{i}^{2}-n\overline{x}^{2}}
\end{align*}
\begin{itemize}
    \item $y = \hat{\beta_{0}}+\hat{\beta_{1}}x$ is called the estimated regression line or least-squares line
\end{itemize}
\newpage
\subsection{Residuals and fitted values}
\begin{itemize}
    \item Residual, $y_{i}-\hat{y_{i}}$
    \begin{itemize}[label=$\circ$]
        \item The difference between the observed value $y_{i}$ and the fitted value $\hat{y_{i}}$
        \item Positive residual $\rightarrow{}$ observed point lies above the least-squares line
        \item Negative residual $\rightarrow{}$ observed point lies below the least-squares line
    \end{itemize}
    \item Sum of residuals, $\displaystyle y_{i}-\hat{y_{i}}$
    \begin{itemize}[label=$\circ$]
        \item For an estimated regression line obtained by the least-squares method,\\ the sum of residuals is zero:
        $$
        \sum_{i=1}^{n} y_{i}-\hat{y_{i}} = 0
        $$
    \end{itemize}
    \item Fitted values $\hat{y_{i}}$
    \begin{itemize}[label=$\circ$]
        \item Obtained by substituting $x_{i}$ into the regression line equation:
        $$
        \hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}x_{i}
        $$
    \end{itemize}
\end{itemize}
\subsection{The simple linear regression model}
\begin{itemize}
    \item The simple linear regression model can be described by the model equation
    $$ Y = \beta_{0} + \beta_{1}x + \varepsilon$$
    where $\varepsilon$ represents uncertainty of the model and is a normal N(0, $\sigma^2$) RV.
    \item The line $y = \beta_{0} + \beta_{1}x$ is called the true/population regression line.
    \item Mean of Y, $E(Y)$
    \begin{align*}
        E(Y) &= E(\beta_{0} + \beta_{1}x + \varepsilon)\\
        &= \beta_{0} + \beta_{1}x + E(\varepsilon)\\
        &= \beta_{0} + \beta_{1}x
    \end{align*}
    \item Variance of Y, $V(Y)$
    \begin{align*}
        V(Y) &= V(\beta_{0} + \beta_{1}x + \varepsilon)\\
        &= 0 + V(\varepsilon)\\
        &= \sigma^2
    \end{align*}
\end{itemize}
\newpage
\subsection{Sum of squared error (SSE)}
\begin{align*}
    SSE = \sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2} = \sum_{i=1}^{n}\left[y_{i}-(\hat{\beta_{0}}+\hat{\beta_{1}}x_{i})\right]^{2}
\end{align*}
\begin{itemize}
    \item Measures discrepancy between the data and the estimation model
    \item Small SSE $\rightarrow{}$ tight fit of estimation model to data
\end{itemize}
\subsection{Estimating \texorpdfstring{$\sigma^2$}{variance} of regression model}
\begin{itemize}
    \item An unbiased estimate for $\sigma^2$ in the regression model is $s^2$:
    $$
    s^2 = \frac{SSE}{n-2} = \frac{\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}}{n-2}
    $$
    \item Estimating $\beta_{0}$ and $\beta_{1}$ results in the loss of 2 degrees of freedom
    \begin{itemize}[label=$\circ$]
        \item Thus the denominator for $s^2$ is $n-2$
    \end{itemize}
\end{itemize}
\end{document}