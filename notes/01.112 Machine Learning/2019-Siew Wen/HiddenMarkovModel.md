# Hidden Markov Model (HMM) <!-- omit in toc -->

Table of Contents
- [Structured Prediction](#structured-prediction)
- [Computing Joint Likelihood of a HMM](#computing-joint-likelihood-of-a-hmm)
- [Supervised Learning](#supervised-learning)
  - [Decoding](#decoding)
    - [Brute Force Enumeration](#brute-force-enumeration)
    - [Viterbi Algorithm](#viterbi-algorithm)
- [Unsupervised Learning](#unsupervised-learning)
  - [Hard EM](#hard-em)
    - [E-step](#e-step)
    - [M-step](#m-step)
  - [Soft EM](#soft-em)
    - [Inference](#inference)
- [Forward-Backward Algorithm](#forward-backward-algorithm)
  - [Going Forward](#going-forward)
  - [Going Backward](#going-backward)

## Structured Prediction

$$ f: S_1 \rightarrow S_2$$
$$
\begin{aligned}
   \text{where } & dim(S_1) > dim(S_2) \\
   & S_1 \text{ is a structured space consisting of word/observation sequences} \\
   & S_2 \text{ is a structured space consisting tag/state sequences}
\end{aligned}
$$

## Computing Joint Likelihood of a HMM

To calculate the probabilities of the sequence of words ($x_0...x_n$) generated given the tags ($y_0...y_{n+1}$)

$$ p(x_1...x_n, y_0...y_{n+1}) = p(y_0...y_{n+1}) \cdot p(x_1...x_n | y_0 ... y_{n+1}) $$


We assumed strong independence between the variables such that 

$$ p(y_2 | y_1, y_0) \approx p(y_2 | y_1) $$


Expanding the joint probabilitiy terms individually,
$$ 
\begin{aligned}
  p(y_0 ... y_{n+1}) =& \prod_{j=0}^n p(y_n+1 | y_n) \\
  p(x_1...x_n | y_0 ... y_n+1) =& \prod_{j=1}^n p(x_j|y_j) \\
  \text{Hence, } p(x_1...x_n, y_0...y_{n+1}) =& \prod_{j=0}^n p(y_n+1 | y_n) \cdot \prod_{j=1}^n p(x_j|y_j) \\
\end{aligned}
$$

For simplicity, we rewrite the notation of individual transmission and emission probabilities.


$$
\begin{aligned}
  \text{Transmission Probability } && a_{u,v} &= \frac{count(u,v)}{count(u)} \\
  \text{Emission Probability } && b_{u(o)} &= \frac{count(u\rightarrow o)}{count(u)} \\
\end{aligned}
$$

$$ 
\begin{aligned}
  \prod_{j=0}^n a_{y_j,y_j+1} &= \prod_{j=0}^n p(y_n+1 | y_n) \\
  \prod_{j=1}^n b_{y_j(x_j)} &= \prod_{j=1}^n p(x_j|y_j) \\
  \text{Joint Probability} &\leftarrow \prod_{j=0}^n a_{y_j,y_j+1} \cdot \prod_{j=1}^n b_{y_j(x_j)}
\end{aligned}
$$

## Supervised Learning

### Decoding
Finding most probable label sequence $y$ given the word sequence $x$

#### Brute Force Enumeration
$$ 
\begin{aligned}
  y* &= \argmax_y p(y|x) \\
  &= \argmax_y p(x,y)/p(x) \\
  &= \argmax_y p(x,y)
\end{aligned}
$$

This method is not feasible once there are too many label sequences. There will be $O(|T|^n)$ possible sequences.


#### Viterbi Algorithm

Since the HMM has a simple dependence structure, we can exploit this in a dynamic programming algorithm.

1. We initialize $\pi(0,u)$
   - $1$ if $u = START$
   - $0$ otherwise
2. For $j=0...n-1$,
$$ 
\pi (j+1,u) = \max_v \{ \pi(j,v) \cdot b_u(x_{j+1}) \cdot a_{v,u} \} 
y_n^* =\argmax_u \{ \pi(j,u) \cdot a_u,y^*_{j+1}\}
$$
3. Finally,
$$ 
\pi (n+1,STOP) = \max_v \{ \pi(n,v)  \cdot a_{v,STOP}) \} 
$$

**Time complexity**: $O((n-1)T^2+T=T) = O(nT^2)$
- T is the number of nodes in each column, and we carry out n-1 operations for  each column.
- Calculation for each node is $O(T)$, and do it for $n*t$ nodes so it is $O(T^2)$.
- $O(T)$ calculation at start and stop nodes.

**Space complexity**: $O(nT)$

## Unsupervised Learning

We can use EM algorithms to learn model parameters in an unsupervised manner.
> Recall that EM Algorithms seek to maximize likelihood that the data is generated by a mixture model.

### Hard EM

#### E-step 

1. Assign each observed sequence of outputs (data point) as $x^{(1)}...x^{(m)}$
2. Assign each $x$ to a single state sequence/tag sequence $y$ *(no partial membership)*.  
3. *Decoding:* For each observation sequence, use Viterbi algorithm to find the most probable state sequence $\textbf{y}^{(i)^*}$

$$ Y^* =\argmax_Y P(Y|X) $$

#### M-step

Parameter estimation using MLE with labeled data from the E-step
$$
\begin{aligned}
  \text{Transmission Probability } && a_{u,v} &= \frac{count(u,v)}{count(u)} && \text{for any } u,v \in \{ 0,1,2...N+1\} \\
  \text{Emission Probability } && b_{u(o)} &= \frac{count(u\rightarrow o)}{count(u)} && \text{for any } u \in \{ 0,1,2...N+1\}, o \in \sum \\
\end{aligned}
$$

### Soft EM

For soft EM, we must evaluate a posterior probability over possible tag sequences, so we cannot use Viterbi algorithm. 
- Viterbi algorithm will result in hard membership (a specific $y$ with max probability) for each observed sequence of outputs.


#### Inference 
To compute the posterior possibilities efficiently for any $u\in 1...N, j\in 1...n$,

$$
p(y_j=u|\textbf{x};\theta) = \sum_{\textbf{y}:y_j=u}p(\textbf{y}|\textbf{x};\theta)
$$
These probabilities is conditioned on the observed sequence $\textbf{x}$ and the model parameters $\theta$.



later
$$ 
\begin{aligned}
  \text{Given that } &\sum_B P(A,B) = P(A) \\
  & \alpha_u(j) = p(x_1...x_{j-1}, y_j=u) \\
  & \beta_u(j) = p(x_j...x_{n} | y_j=u) \\
\end{aligned}
$$

$$ 
\begin{aligned}
  p(y_j = u | \textbf{x};\theta) &= \frac{\alpha_u(j)\beta_u(j)}{\sum_v \alpha_v(k)\beta_v(k)} \\
  p(y_j = u, y_{j+1} = v|\textbf{x};\theta) &= \frac{\alpha_u(j)\cdot b_u{x_j} \cdot a_{u,v} \cdot \beta_v(j+1)}{\sum_v \alpha_v(k)\beta_v(k)}
\end{aligned}
$$

**Finding the fractional count**
$$ 
\begin{aligned}
  count(u,v) &= \sum_{i=1}^m count^{(i)} (u,v) \\
  &= \sum_{i=1}^m \sum_y p(y|x^{(i)}) count(\textbf{x}^{(i)},y, u \rightarrow v) \\
  &= \sum_{i=1}^m \sum_{j=0}^n p(y_j = u, y_{j+1} = v|\textbf{x}^{(i)}) \\
  &= \sum_{i=1}^m \sum_{j=0}^n \frac{\alpha_u(j)\cdot b_u{x_j} \cdot a_{u,v} \cdot \beta_v(j+1)}{\sum_v \alpha_v(k)\beta_v(k)}
\end{aligned}
$$

## Forward-Backward Algorithm

### Going Forward

- $\alpha_u(j) =$ the sum of the scores of all paths from START to node $u$ at $j$

- For the first node, there is no emission probability. 
$$
\begin{aligned}
  \alpha_u(1) &= \alpha_{START,u} \\ 
  &= p(x_0 , y_1 = u)
\end{aligned}
$$

- For the other nodes,

$$\text{Given that }\sum_B P(A,B) = P(A)$$

$$
\begin{aligned}
  \alpha_u(j+1) &= p(x_1 ... x_j, y_{j+1} = u) \\
  &= \sum_v p(x_1...x_j, y_{j+1} = u, y_j = v) \\
  &= \sum_v p(x_1...x_{j-1}, y_j = v, x_j, y_{j+1}= u) \\
  &= \sum_v p(x_1...x_{j-1}, y_j = v) \cdot b_v(x_j) \cdot a_{v,u} \\
  &= \sum_v \alpha_v(j) \cdot b_v(x_j)\cdot \alpha_{v,u}
\end{aligned}
$$

### Going Backward

- $\beta_u(j) =$ the sum of the scores of all paths from node $u$ at $j$ to STOP.

$$ 
\begin{aligned}
  \beta_u(j) &= p(x_j...x_n | y_j = u) \\
  &= \sum_v p(x_j...x_n, y_{j+1} = v | y_j = u) \\
  &= \sum_v p(x_j, y_{j+1} = v, x_{j+1}... x_n | y_j =u) \\
  &= \sum_v b_u(x_j) \cdot a_{u,v} \cdot p(x_{j+1}...x_n | y_{j+1} = v) \\
  &= \sum_v b_u(x_j) \cdot a_{u,v} \cdot \beta_v(j+1)
\end{aligned}
$$

Time Complexity: $O(nT^2)$
- $O(T)$ operations for $T$ nodes at $n$ positions

Time complexity of 1 EM iteration: $O(mnT^2)$

Forward Backward Algorithm is a type of Inside-Outside Algorithm, which the Prof will only cover in NLP course.
